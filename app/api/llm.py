"""
LLMç›¸å…³APIè·¯ç”±

æä¾›LLMå¯¹è¯ã€æ¨¡å‹ç®¡ç†ç­‰åŠŸèƒ½æ¥å£ã€‚
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from typing import Optional
import json
import httpx
from app.models.database import get_db
from app.models.schemas import (
    ChatRequest,
    ChatResponse,
    ApiResponse,
    ModelConfigCreate,
    ModelConfigUpdate,
    ModelConfigResponse
)
from app.core.llm_service import llm_service
from app.models.database import ModelConfig
from app.utils.logger import setup_logger

logger = setup_logger(__name__)

router = APIRouter(prefix="/llm", tags=["LLMç®¡ç†"])


@router.post("/chat")
async def chat(request: ChatRequest):
    """
    å¯¹è¯æ¥å£ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
    
    å‘é€å¯¹è¯è¯·æ±‚ã€‚æ ¹æ®streamå‚æ•°è¿”å›å®Œæ•´å›å¤æˆ–æµå¼è¾“å‡ºã€‚
    
    Args:
        request: å¯¹è¯è¯·æ±‚ï¼ŒåŒ…å«æ¶ˆæ¯åˆ—è¡¨å’Œé…ç½®é€‰é¡¹
    
    Returns:
        å¦‚æœstream=Falseï¼Œè¿”å›ChatResponseï¼ˆå®Œæ•´å›å¤ï¼‰
        å¦‚æœstream=Trueï¼Œè¿”å›StreamingResponseï¼ˆSSEæµå¼è¾“å‡ºï¼‰
    
    Examples:
        POST /api/v1/llm/chat
        {
            "messages": [
                {"role": "user", "content": "ä½ å¥½"}
            ],
            "model_config_name": "default",
            "stream": true
        }
    """
    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = [{"role": msg.role.value, "content": msg.content} for msg in request.messages]
        
        # å¦‚æœè¯·æ±‚æµå¼è¾“å‡º
        if request.stream:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = [{"role": msg.role.value, "content": msg.content} for msg in request.messages]
            
            # è·å–æ¨¡å‹é…ç½®
            from app.models.database import SessionLocal
            from app.models.database import ModelConfig
            db = SessionLocal()
            
            try:
                # è·å–æ¨¡å‹é…ç½®
                if request.model_config_name:
                    model_config = db.query(ModelConfig).filter(
                        ModelConfig.name == request.model_config_name,
                        ModelConfig.is_active == True
                    ).first()
                else:
                    model_config = db.query(ModelConfig).filter(
                        ModelConfig.is_default == True,
                        ModelConfig.is_active == True
                    ).first()
                
                # å‡†å¤‡è°ƒç”¨å‚æ•°
                chat_params = {
                    "messages": messages,
                    "temperature": 0.7,
                    "max_tokens": 2000,
                    "top_p": 0.9,
                    "stream": True
                }
                
                if model_config:
                    chat_params["model"] = model_config.model_name
                    chat_params["temperature"] = model_config.temperature
                    chat_params["max_tokens"] = model_config.max_tokens
                    chat_params["top_p"] = model_config.top_p
                else:
                    chat_params["model"] = "glm-4"
                
                # è·å–ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºæç¤ºè¯æ¨¡æ¿ï¼‰
                user_message = ""
                for msg in reversed(messages):
                    if msg.get("role") == "user":
                        user_message = msg.get("content", "")
                        break
                
                # å¦‚æœæœ‰çŸ¥è¯†åº“ï¼Œä½¿ç”¨RAGæŸ¥è¯¢
                if request.knowledge_base_name:
                    if user_message:
                        from app.core.agentic_rag import AgenticRAG, AgenticRAGConfig
                        # ç®€åŒ–é…ç½®ä»¥å‡å°‘è¶…æ—¶
                        rag_config = AgenticRAGConfig(
                            enable_task_decomposition=False,  # ç¦ç”¨ä»»åŠ¡åˆ†è§£ä»¥åŠ å¿«é€Ÿåº¦
                            enable_self_reflection=False,     # ç¦ç”¨è‡ªåæ€ä»¥åŠ å¿«é€Ÿåº¦
                            max_retrieval_rounds=1            # å‡å°‘æ£€ç´¢è½®æ¬¡
                        )
                        rag = AgenticRAG(config=rag_config)
                        rag_result = rag.query(
                            query=user_message,
                            knowledge_base_name=request.knowledge_base_name
                        )
                        
                        # æ„å»ºå‚è€ƒä¿¡æ¯å­—ç¬¦ä¸²
                        context_items = []
                        if rag_result.get("sources"):
                            for i, source in enumerate(rag_result.get("sources", [])[:3], 1):
                                context_items.append(f"[å‚è€ƒ{i}] {source}")
                        context_str = "\n".join(context_items) if context_items else ""
                        
                        # ä¿å­˜æ€è€ƒè¿‡ç¨‹ä¾›åç»­å‘é€
                        reasoning_trace = rag_result.get("reasoning_trace", [])
                        query_type = rag_result.get("query_type", "unknown")
                        strategy = rag_result.get("strategy", "hybrid")
                    else:
                        context_str = ""
                        reasoning_trace = []
                        query_type = "unknown"
                        strategy = "hybrid"
                    
                    # è·å–å½“å‰å¯ç”¨çš„æç¤ºè¯æ¨¡æ¿
                    from app.core.prompt_manager import prompt_manager
                    active_template = prompt_manager.get_active_template(db)
                    
                    if active_template:
                        import json
                        template_variables = json.loads(active_template.variables) if active_template.variables else []
                        
                        # æ„å»ºå˜é‡å­—å…¸
                        variables = {"requirement": user_message}
                        if "context" in template_variables and context_str:
                            variables["context"] = context_str
                        
                        # ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–æç¤ºè¯
                        formatted_result = prompt_manager.format_prompt(
                            db,
                            active_template.name,
                            variables
                        )
                        
                        # ä½¿ç”¨æ ¼å¼åŒ–åçš„æ¶ˆæ¯
                        enhanced_messages = [
                            {"role": "system", "content": formatted_result.system},
                            {"role": "user", "content": formatted_result.user}
                        ]
                        chat_params["messages"] = enhanced_messages
                    else:
                        # æ²¡æœ‰å¯ç”¨çš„æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨ç®€å•é€»è¾‘
                        enhanced_messages = messages.copy()
                        if context_str:
                            enhanced_messages[-1]["content"] = (
                                f"å‚è€ƒä¿¡æ¯:\n{context_str}\n\n"
                                f"ç”¨æˆ·é—®é¢˜: {user_message}\n\n"
                                f"è¯·æ ¹æ®å‚è€ƒä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                            )
                        chat_params["messages"] = enhanced_messages
                    
                    db.close()
                    
                    # åˆ›å»ºè‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼Œè®¾ç½®æ›´é•¿è¶…æ—¶æ—¶é—´
                    import httpx
                    timeout_config = httpx.Timeout(timeout=600.0, connect=10.0)
                    
                    from zhipuai import ZhipuAI
                    if model_config and model_config.api_key:
                        client_kwargs = {"api_key": model_config.api_key, "timeout": timeout_config}
                        if model_config.api_base:
                            client_kwargs["base_url"] = model_config.api_base
                        client = ZhipuAI(**client_kwargs)
                    else:
                        from app.config import settings
                        client = ZhipuAI(api_key=settings.zhipuai_api_key, timeout=timeout_config)
                    
                    # è°ƒç”¨APIå¹¶æµå¼è¿”å›
                    response = client.chat.completions.create(**chat_params)
                    
                    async def generate():
                        # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œå…ˆå‘é€æ€è€ƒè¿‡ç¨‹
                        if reasoning_trace:
                            thinking_content = "ğŸ¤” **æ€è€ƒè¿‡ç¨‹ï¼š**\n\n"
                            
                            # æ·»åŠ æŸ¥è¯¢è·¯ç”±ä¿¡æ¯
                            query_type_map = {
                                "FACTUAL": "äº‹å®æ€§æŸ¥è¯¢",
                                "CONCEPTUAL": "æ¦‚å¿µæ€§æŸ¥è¯¢",
                                "PROCEDURAL": "ç¨‹åºæ€§æŸ¥è¯¢",
                                "ANALYTICAL": "åˆ†ææ€§æŸ¥è¯¢",
                                "EXPLORATORY": "æ¢ç´¢æ€§æŸ¥è¯¢"
                            }
                            
                            if query_type:
                                thinking_content += f"ğŸ“‹ **æŸ¥è¯¢ç±»å‹ï¼š** {query_type_map.get(query_type, query_type)}\n\n"
                            
                            # æ·»åŠ æ£€ç´¢ç­–ç•¥
                            strategy_map = {
                                "semantic": "è¯­ä¹‰æ£€ç´¢",
                                "keyword": "å…³é”®è¯æ£€ç´¢",
                                "hybrid": "æ··åˆæ£€ç´¢"
                            }
                            if strategy:
                                thinking_content += f"ğŸ” **æ£€ç´¢ç­–ç•¥ï¼š** {strategy_map.get(strategy, strategy)}\n\n"
                            
                            # æ·»åŠ æ¨ç†è½¨è¿¹
                            for trace in reasoning_trace:
                                step_name = trace.get("step", "")
                                result = trace.get("result", {})
                                
                                if step_name == "query_routing":
                                    thinking_content += f"ğŸ¯ **æŸ¥è¯¢è·¯ç”±ï¼š** åˆ†æå®Œæˆ\n\n"
                                elif step_name == "task_decomposition":
                                    subtasks = result.get("subtasks", [])
                                    if subtasks:
                                        thinking_content += f"ğŸ“ **ä»»åŠ¡åˆ†è§£ï¼š**\n"
                                        for i, subtask in enumerate(subtasks, 1):
                                            thinking_content += f"  {i}. {subtask.get('task', '')}\n"
                                        thinking_content += "\n"
                                elif step_name == "retrieval":
                                    thinking_content += f"ğŸ“š **çŸ¥è¯†æ£€ç´¢ï¼š** æ£€ç´¢åˆ° {len(result.get('sources', []))} æ¡ç›¸å…³å†…å®¹\n\n"
                            
                            thinking_content += "---\n\n"
                            
                            yield f"data: {json.dumps({'content': thinking_content, 'type': 'thinking'}, ensure_ascii=False)}\n\n"
                        
                        # å‘é€å®é™…å›å¤å†…å®¹
                        for chunk in response:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                yield f"data: {json.dumps({'content': content, 'type': 'response'}, ensure_ascii=False)}\n\n"
                        yield "data: [DONE]\n\n"
                    
                    return StreamingResponse(generate(), media_type="text/event-stream")
                
            finally:
                db.close()
        else:
            # éæµå¼è¾“å‡ºï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            result = llm_service.chat_with_config(
                messages=messages,
                config_name=request.model_config_name,
                stream=request.stream,
                knowledge_base_name=request.knowledge_base_name
            )
            
            return ChatResponse(
                content=result["content"],
                model=result["model"],
                usage=result["usage"]
            )
        
    except Exception as e:
        logger.error(f"å¯¹è¯å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¯¹è¯å¤±è´¥: {str(e)}"
        )


@router.get("/models")
async def list_models():
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
    
    Returns:
        æ¨¡å‹åˆ—è¡¨ï¼ŒåŒ…å«æ¨¡å‹åç§°å’Œæè¿°
    
    Examples:
        GET /api/v1/llm/models
    """
    try:
        models = llm_service.list_models()
        return ApiResponse(
            success=True,
            message="è·å–æ¨¡å‹åˆ—è¡¨æˆåŠŸ",
            data=models
        )
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}"
        )


@router.post("/config", response_model=ModelConfigResponse)
async def create_model_config(
    config: ModelConfigCreate,
    db: Session = Depends(get_db)
):
    """
    åˆ›å»ºæ¨¡å‹é…ç½®
    
    åˆ›å»ºæ–°çš„æ¨¡å‹é…ç½®ï¼Œå¯ä»¥è®¾ç½®temperatureã€max_tokensç­‰å‚æ•°ã€‚
    
    Args:
        config: æ¨¡å‹é…ç½®æ•°æ®
    
    Returns:
        åˆ›å»ºçš„æ¨¡å‹é…ç½®
    
    Examples:
        POST /api/v1/llm/config
        {
            "name": "creative",
            "model_name": "glm-4",
            "temperature": 1.2,
            "max_tokens": 3000
        }
    """
    try:
        # æ£€æŸ¥é…ç½®åæ˜¯å¦å·²å­˜åœ¨
        existing = db.query(ModelConfig).filter(ModelConfig.name == config.name).first()
        if existing:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"é…ç½®åç§°å·²å­˜åœ¨: {config.name}"
            )
        
        # å¦‚æœè®¾ç½®ä¸ºé»˜è®¤ï¼Œå–æ¶ˆå…¶ä»–é»˜è®¤é…ç½®
        if config.is_default:
            db.query(ModelConfig).filter(ModelConfig.is_default == True).update({"is_default": False})
        
        # åˆ›å»ºé…ç½®
        model_config = ModelConfig(**config.model_dump())
        db.add(model_config)
        db.commit()
        db.refresh(model_config)
        
        logger.info(f"åˆ›å»ºæ¨¡å‹é…ç½®æˆåŠŸ: {config.name}")
        
        return ModelConfigResponse.model_validate(model_config)
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ›å»ºæ¨¡å‹é…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ›å»ºæ¨¡å‹é…ç½®å¤±è´¥: {str(e)}"
        )


@router.get("/config", response_model=list[ModelConfigResponse])
async def list_model_configs(
    skip: int = 0,
    limit: int = 100,
    is_active: Optional[bool] = None,
    db: Session = Depends(get_db)
):
    """
    åˆ—å‡ºæ¨¡å‹é…ç½®
    
    æ”¯æŒåˆ†é¡µå’ŒçŠ¶æ€ç­›é€‰ã€‚
    
    Args:
        skip: è·³è¿‡çš„è®°å½•æ•°
        limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°
        is_active: æ˜¯å¦å¯ç”¨ç­›é€‰
    
    Returns:
        æ¨¡å‹é…ç½®åˆ—è¡¨
    
    Examples:
        GET /api/v1/llm/config?skip=0&limit=10&is_active=true
    """
    try:
        query = db.query(ModelConfig)
        
        if is_active is not None:
            query = query.filter(ModelConfig.is_active == is_active)
        
        configs = query.offset(skip).limit(limit).all()
        
        return [ModelConfigResponse.model_validate(c) for c in configs]
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºæ¨¡å‹é…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ—å‡ºæ¨¡å‹é…ç½®å¤±è´¥: {str(e)}"
        )


@router.get("/config/{config_id}", response_model=ModelConfigResponse)
async def get_model_config(
    config_id: int,
    db: Session = Depends(get_db)
):
    """
    è·å–æ¨¡å‹é…ç½®è¯¦æƒ…
    
    Args:
        config_id: é…ç½®ID
    
    Returns:
        æ¨¡å‹é…ç½®è¯¦æƒ…
    
    Examples:
        GET /api/v1/llm/config/1
    """
    try:
        config = db.query(ModelConfig).filter(ModelConfig.id == config_id).first()
        if not config:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"é…ç½®ä¸å­˜åœ¨: {config_id}"
            )
        
        return ModelConfigResponse.model_validate(config)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}"
        )


@router.put("/config/{config_id}", response_model=ModelConfigResponse)
async def update_model_config(
    config_id: int,
    update_data: ModelConfigUpdate,
    db: Session = Depends(get_db)
):
    """
    æ›´æ–°æ¨¡å‹é…ç½®
    
    Args:
        config_id: é…ç½®ID
        update_data: æ›´æ–°æ•°æ®
    
    Returns:
        æ›´æ–°åçš„æ¨¡å‹é…ç½®
    
    Examples:
        PUT /api/v1/llm/config/1
        {
            "temperature": 0.8,
            "max_tokens": 2500
        }
    """
    try:
        config = db.query(ModelConfig).filter(ModelConfig.id == config_id).first()
        if not config:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"é…ç½®ä¸å­˜åœ¨: {config_id}"
            )
        
        # æ›´æ–°å­—æ®µ
        update_dict = update_data.model_dump(exclude_unset=True)
        
        # å¦‚æœè®¾ç½®ä¸ºé»˜è®¤ï¼Œå–æ¶ˆå…¶ä»–é»˜è®¤é…ç½®
        if update_dict.get("is_default") == True:
            db.query(ModelConfig).filter(ModelConfig.id != config_id).filter(ModelConfig.is_default == True).update({"is_default": False})
        
        for key, value in update_dict.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        db.commit()
        db.refresh(config)
        
        logger.info(f"æ›´æ–°æ¨¡å‹é…ç½®æˆåŠŸ: {config_id}")
        
        return ModelConfigResponse.model_validate(config)
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"æ›´æ–°æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ›´æ–°æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}"
        )


@router.delete("/config/{config_id}")
async def delete_model_config(
    config_id: int,
    db: Session = Depends(get_db)
):
    """
    åˆ é™¤æ¨¡å‹é…ç½®
    
    Args:
        config_id: é…ç½®ID
    
    Returns:
        åˆ é™¤ç»“æœ
    
    Examples:
        DELETE /api/v1/llm/config/1
    """
    try:
        config = db.query(ModelConfig).filter(ModelConfig.id == config_id).first()
        if not config:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"é…ç½®ä¸å­˜åœ¨: {config_id}"
            )
        
        db.delete(config)
        db.commit()
        
        logger.info(f"åˆ é™¤æ¨¡å‹é…ç½®æˆåŠŸ: {config_id}")
        
        return ApiResponse(
            success=True,
            message=f"é…ç½®åˆ é™¤æˆåŠŸ: {config_id}"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"åˆ é™¤æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ é™¤æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}"
        )
