# 免费嵌入模型使用说明

## 🎉 好消息！

现在支持**完全免费**的本地嵌入模型，无需任何API密钥或充值！

## 免费模型列表

### 🇨🇳 中文模型（推荐）

#### 1. BGE-small-zh-v1.5（快速）
- **模型标识**：`local-BAAI/bge-small-zh-v1.5`
- **特点**：
  - 快速轻量
  - 适合中文文本
  - 资源占用低
  - 速度：~1000文档/分钟
- **推荐场景**：快速测试、大量文档处理

#### 2. BGE-base-zh-v1.5（平衡）⭐ 推荐
- **模型标识**：`local-BAAI/bge-base-zh-v1.5`
- **特点**：
  - 速度和精度平衡
  - 优秀的中文语义理解
  - 适合大多数应用
  - 速度：~600文档/分钟
- **推荐场景**：生产环境、日常使用

#### 3. BGE-large-zh-v1.5（高精度）
- **模型标识**：`local-BAAI/bge-large-zh-v1.5`
- **特点**：
  - 最高精度
  - 深度语义理解
  - 资源占用较高
  - 速度：~300文档/分钟
- **推荐场景**：高质量检索、复杂查询

### 🌍 多语言模型

#### 4. MiniLM-L6-v2（多语言）
- **模型标识**：`local-sentence-transformers/all-MiniLM-L6-v2`
- **特点**：
  - 支持100+语言
  - 极速处理
  - 轻量级
  - 速度：~1500文档/分钟
- **推荐场景**：多语言文档、快速原型

## 使用步骤

### 第一步：安装依赖库

首次使用本地模型需要安装依赖库：

```bash
pip install llama-index-embeddings-huggingface
pip install sentence-transformers
pip install transformers
pip install torch
```

或者使用requirements.txt更新：

```bash
pip install -r requirements.txt
```

**注意**：
- PyTorch安装包较大（~2GB），首次安装需要时间
- 如果安装缓慢，可以使用国内镜像源：
  ```bash
  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torch
  ```

### 第二步：选择本地免费模型

在"添加文档"或"创建知识库"时：

1. 在"嵌入模型"下拉框中
2. 选择"🆓 本地免费模型"分组
3. 推荐选择 **BGE-base-zh-v1.5（中文，平衡）**
4. 无需任何API密钥

### 第三步：正常使用

选择本地模型后，使用方式与付费模型完全相同：

1. 上传文档
2. 配置参数
3. 点击"添加文档到知识库"
4. 等待处理完成

## 性能对比

| 模型 | 速度 | 精度 | 成本 | API密钥 | 中文支持 |
|------|------|------|------|---------|----------|
| **BGE-small-zh-v1.5** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 免费 | ❌ | ✅ 优秀 |
| **BGE-base-zh-v1.5** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 免费 | ❌ | ✅ 优秀 |
| **BGE-large-zh-v1.5** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 免费 | ❌ | ✅ 优秀 |
| MiniLM-L6-v2 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 免费 | ❌ | ⚠️ 一般 |
| zhipuai-embedding | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 付费 | ✅ | ✅ 优秀 |
| text-embedding-3-small | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 付费 | ✅ | ✅ 良好 |

## 系统要求

### 最低配置
- CPU：双核处理器
- 内存：4GB
- 硬盘：5GB可用空间

### 推荐配置
- CPU：四核或以上
- 内存：8GB或以上
- 硬盘：10GB可用空间

### 首次使用
- 首次使用会自动下载模型文件（~500MB-1GB）
- 下载时间取决于网络速度
- 模型会缓存在本地，无需重复下载

## 常见问题

### Q1: 为什么推荐本地免费模型？
**A**:
1. ✅ 完全免费，无需充值
2. ✅ 无需API密钥
3. ✅ 数据隐私保护（本地处理）
4. ✅ 中文效果优秀
5. ✅ 速度和精度平衡

### Q2: 本地模型效果如何？
**A**: BGE系列模型在中文语义理解上表现优秀，多次在中文语义检索评测中获得第一名。对于大多数应用场景，效果与付费模型相当甚至更好。

### Q3: 会不会很慢？
**A**: 
- BGE-small: ~1000文档/分钟
- BGE-base: ~600文档/分钟
- 对于常规使用，速度完全可接受

### Q4: 需要GPU吗？
**A**: 不需要。CPU模式即可满足大多数需求。如果有GPU，速度会更快。

### Q5: 如何切换模型？
**A**: 在创建知识库或添加文档时，选择不同的嵌入模型即可。注意：不同模型创建的知识库不能混用。

### Q6: 可以混合使用吗？
**A**: 不建议。同一个知识库应该使用相同的嵌入模型，否则会影响检索效果。

## 技术支持

如果遇到问题：
1. 检查是否正确安装依赖库
2. 查看浏览器控制台日志（F12）
3. 查看服务器终端日志
4. 尝试重启服务器

## 推荐配置

### 快速测试
```
模型: BGE-small-zh-v1.5
分块大小: 512
分块重叠: 50
```

### 生产环境
```
模型: BGE-base-zh-v1.5（推荐）
分块大小: 512
分块重叠: 50
```

### 高精度需求
```
模型: BGE-large-zh-v1.5
分块大小: 1024
分块重叠: 100
```